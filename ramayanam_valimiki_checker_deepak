import pandas as pd
import torch
from sentence_transformers import SentenceTransformer, util
from g4f.client import Client
import os, time

# === Config ===

SLEEP_SECONDS = 8

# === Load Ramayanam Data ===
df = pd.read_excel("merged_sorted_by_chapter_verse.xlsx")
df = df.dropna(subset=["translation"]).reset_index(drop=True)
texts = df["translation"].astype(str).tolist()

# === Load Fast Model ===

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline


model_id = "google/flan-t5-xl"  

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map="auto")
# === Get Most Relevant Verse for Statement ===
def get_best_match(statement):
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np

    # Load and clean the dataset again
    df = pd.read_excel("merged_sorted_by_chapter_verse.xlsx")
    texts = df['translation'].dropna().tolist()

    # Load the powerful yet fast model
    model = SentenceTransformer("paraphrase-MiniLM-L3-v2")

    # Encode all translations
    corpus_embeddings = model.encode(texts, convert_to_numpy=True, batch_size=32, show_progress_bar=True)

    # Encode the input sentence
    input_sentence = statement
    query_embedding = model.encode([input_sentence], convert_to_numpy=True)

    # Compute cosine similarity
    similarities = cosine_similarity(query_embedding, corpus_embeddings)[0]

    # Get top 55 most similar indices
    top_indices = np.argsort(similarities)[-55:][::-1]
    top_sentences = [texts[i] for i in top_indices]

    # Combine into a single string
    grouped_string = "\n\n".join(top_sentences)

    # Return preview and count
    return grouped_string[:500]


# === g4f Verifier with Best Match ===
# === g4f Verifier with Best Match ===
def gpt_verifier(statements):
    client = Client()
    predictions = []

    for i, stmt in enumerate(statements):
            best_text= get_best_match(stmt)
            qa_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer)

            context = f"{best_text}"

            question = str(stmt)

            prompt = f"Answer the question based on the passage:\n\nContext: {context}\n\nQuestion: {question} say true or false only"
            response = qa_pipeline(prompt, max_new_tokens=50, truncation=True)
            print(response[0]["generated_text"])

            result = str(response[0]["generated_text"]).strip().lower()
            prediction = "True" if "true" in result and "false" not in result else "False"


            predictions.append(prediction)
            print(f"[{i+1}/{len(statements)}] Prediction: {prediction} -> {stmt[:50]}...")
            time.sleep(SLEEP_SECONDS)

    return predictions# === MAIN EXECUTION ===
if __name__ == "__main__":
    # Load the evaluation dataset
    gpt_df = pd.read_csv("Valmiki Ramanaya Verses - Final Evaluation.csv")
    verified_statements = gpt_df["Statement"].dropna().tolist()

    print("üîç Running hybrid verification (MiniLM + g4f)...")
    preds = gpt_verifier(verified_statements)

    # Save results
    gpt_df = gpt_df.dropna(subset=["Statement"]).copy()
    gpt_df["Prediction"] = preds
    gpt_df.to_csv("Ramayanam_Hybrid_Verified.csv", index=False)
    print("\n‚úÖ Saved: 'Ramayanam_Hybrid_Verified.csv'")   

